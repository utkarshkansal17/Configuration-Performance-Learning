# Configuration Performance Learning

## üîç Objective

To evaluate how effectively different regression models can predict performance outcomes (like execution time or compression ratio) across highly configurable software systems.

## üìä Models Implemented

- Linear Regression (Baseline)
- Neural Network
- Random Forest
- Support Vector Regression (SVR)
- SVR with Hyperparameter Tuning
- Voting Regressor Ensemble
- Voting Regressor with Tuning
- XGBoost Regressor


## üóÇÔ∏è Project Structure

```
Configuration-Performance-Learning/
‚îú‚îÄ‚îÄ Comparison by System.py                # Generates heatmaps to compare models across systems
‚îú‚îÄ‚îÄ NN.py                                  # Implements and trains the Neural Network model
‚îú‚îÄ‚îÄ README.md                              # Project documentation
‚îú‚îÄ‚îÄ Results/                               # Contains outputs from experiments
‚îÇ   ‚îú‚îÄ‚îÄ Comparisons/                       # Consolidated results and system-level visualizations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RES_system2_comparison_average_MAE.csv         # System-wise MAE comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RES_system2_comparison_average_MAPE.csv        # System-wise MAPE comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RES_system2_comparison_average_RMSE.csv        # System-wise RMSE comparison
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RES_system_heatmap2_average_MAE_clean.png      # MAE heatmap
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RES_system_heatmap2_average_MAPE_clean.png     # MAPE heatmap
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ RES_system_heatmap2_average_RMSE_clean.png     # RMSE heatmap
‚îÇ   ‚îú‚îÄ‚îÄ Linear Regression Baseline.csv     # Raw results of linear regression baseline
‚îÇ   ‚îú‚îÄ‚îÄ NN.csv                              # Neural network performance per system
‚îÇ   ‚îú‚îÄ‚îÄ RandomForest.csv                    # Random forest model results
‚îÇ   ‚îú‚îÄ‚îÄ SVM with Tunning.csv                # SVM model with hyperparameter tuning
‚îÇ   ‚îú‚îÄ‚îÄ SVR.csv                             # Standard Support Vector Regression results
‚îÇ   ‚îú‚îÄ‚îÄ Stat_test.ipynb                     # T-test statistical comparison of all models
‚îÇ   ‚îú‚îÄ‚îÄ Voting Regressor EL with Tunning.csv   # Ensemble voting regressor with tuning
‚îÇ   ‚îú‚îÄ‚îÄ Voting Regressor EL.csv             # Ensemble voting regressor without tuning
‚îÇ   ‚îî‚îÄ‚îÄ XGBR.csv                             # XGBoost model predictions
‚îú‚îÄ‚îÄ SVM with GridSearch tunning.py         # SVM with GridSearchCV for best hyperparameters
‚îú‚îÄ‚îÄ SVM with Tunning.py                    # SVM with manual tuning
‚îú‚îÄ‚îÄ SVM.py                                 # Basic SVM model
‚îú‚îÄ‚îÄ Voting Regressor EL.py                 # Implements ensemble learning using voting regressor
‚îú‚îÄ‚îÄ Voting Regressor with Tunning.py       # Voting regressor with feature engineering and tuning
‚îú‚îÄ‚îÄ XGBR.py                                # XGBoost model script
‚îú‚îÄ‚îÄ comparisons.py                         # Compares models and generates average results
‚îú‚îÄ‚îÄ datasets/                              # Input datasets for each system
‚îÇ   ‚îú‚îÄ‚îÄ Irzip/                              # Long-range compression datasets
‚îÇ   ‚îú‚îÄ‚îÄ batlik/                             # Image processing datasets
‚îÇ   ‚îú‚îÄ‚îÄ dconvert/                           # Document conversion test cases
‚îÇ   ‚îú‚îÄ‚îÄ h2/                                 # Database system configuration datasets
‚îÇ   ‚îú‚îÄ‚îÄ jump3r/                             # Audio encoding workloads
‚îÇ   ‚îú‚îÄ‚îÄ kanzi/                              # General compression tool configurations
‚îÇ   ‚îú‚îÄ‚îÄ x264/                               # Video encoding benchmark data
‚îÇ   ‚îú‚îÄ‚îÄ xz/                                 # Compression datasets for xz utility
‚îÇ   ‚îî‚îÄ‚îÄ z3/                                 # SMT theorem prover configuration performance data
‚îú‚îÄ‚îÄ lab2_solution.py                       # Central orchestration for all model training and evaluation
‚îî‚îÄ‚îÄ randomforest.py                        # Script for training and evaluating Random Forest

```
## ‚öôÔ∏è Installation & Usage

### üîß Prerequisites

Make sure you have the following installed:

- Python 3.8 or higher
- pip (Python package manager)
### üõ†Ô∏è Required Libraries

To run the project, install the following Python packages:

- **Data Handling:** `numpy`, `pandas`
- **Machine Learning Models:** `scikit-learn`, `xgboost`, `tensorflow`, `keras`, `skorch`
- **Statistical Analysis:** `scipy`, `statsmodels` *(optional for significance testing)*
- **Visualization:** `matplotlib`, `seaborn`, `altair`
- **Notebooks Support:** `jupyterlab`, `notebook`, `ipykernel`, `nbformat`, `nbconvert`
- **Experiment Tracking:** `wandb`
- **Utilities:** `joblib`, `tqdm`, `tabulate`
- **PyTorch Suite:** `torch`, `torchvision`, `torchaudio`, `torchtext`, `torchdata`

You can install all at once using:

```bash
pip install -r requirements.txt
```

## üöÄ Running the Code

Run any model script:

python XGBR.py  # notedown the output

python "Voting Regressor with Tunning.py" # notedown the output

Generate system-wise comparison heatmaps:

python "Comparison by System.py"

After running the scripts the output will look this

```bash
> System: batlik, Dataset: corona.csv, Training fraction: 0.7, Repeats: 3
Average MAPE: 0.02
Average MAE: 0.04
Average RMSE: 0.05
```


Perform statistical significance tests (via Jupyter Notebook):

jupyter notebook "Results/Stat_test.ipynb"

## üìù Logging Results to CSV

To standardize the evaluation output across all model scripts, use a structured list called `results` to collect and store performance metrics for each dataset within a system.

### üîπ Step-by-Step Logging Guide

1. **Initialize a results list** at the start of your script (before the main loop):

```python
results = []
```
2. After computing average metrics inside your inner loop (i.e., after printing the final scores), log the result for that dataset like this:

```python

# Collect results for CSV export
results.append({ 
    'System': current_system,
    'Dataset': csv_file,
    'TrainFraction': train_frac,
    'Repeats': num_repeats,
    'Average MAPE': avg_mape,
    'Average MAE': avg_mae,
    'Average RMSE': avg_rmse
})
```
At the end of the script, save all results to a CSV file in the Results/ directory:

```python
import pandas as pd
os.makedirs('Results', exist_ok=True)
pd.DataFrame(results).to_csv('Results/YourModelName.csv', index=False)
```
üìå Replace "YourModelName.csv" with the specific filename for your model (e.g., XGBR.csv, Voting Regressor EL with Tunning.csv, etc.).

| System   | Dataset         | TrainFraction | Repeats | Average MAPE | Average MAE | Average RMSE |
|----------|-----------------|----------------|---------|----------------|---------------|----------------|
| batlik   | corona.csv      | 0.7            | 30      | 0.023          | 0.045         | 0.067          |
| dconvert | jpeg-small.csv  | 0.7            | 30      | 0.012          | 0.034         | 0.058          |
| ...      | ...             | ...            | ...     | ...            | ...           | ...            |

You can also check the Results folder csv files for more understanding of the above output results

This logging mechanism ensures reproducibility and simplifies comparison across models and systems. It also enables downstream statistical analysis and visualization (e.g., heatmaps, t-tests).
